{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e487c5",
   "metadata": {},
   "source": [
    "# Experimenting with open LLMs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7658baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2170bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  response = await AsyncClient().chat(model='gemma3', messages=[message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63b2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response=await chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33c0588",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7027ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the path to your .env file\n",
    "# This could be an absolute or relative path\n",
    "env_path = '/Users/himanshu/projects/journalling-assitant/.env' \n",
    "# or relative: env_path = '../config/.env'\n",
    "\n",
    "# Load the .env file from the specified path\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Now you can access the variables as usual\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# print(f\"SECRET_KEY: {secret_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1621018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='**Maya:** Excuse me, is this seat taken?\\n\\n**Alex:** Nope, go ahead. But if I do sit down, you may already know my name—Alex. (He offers her a friendly grin.)\\n\\n**Maya:** Maya. (She closes her notebook, smiles nervously.) I was hoping someone more... I mean, no, I\\'d rather not say what I was hoping you were. My flight was delayed, and I need a place to think.\\n\\n**Alex:** All right. (He pulls out a cup of coffee he ordered earlier.) Here, take a sip. It looks like you have a storm brewing in that pile of papers.\\n\\n**Maya:** (Laughs softly.) It’s a thesis proposal. I keep getting distracted by random thoughts. Do you... do you mind if I keep it—(she glances around—if I keep it here?\\n\\n**Alex:** Not at all. (He nods toward the book that’s open in front of him.) A piece of work? What are you up to?\\n\\n**Maya:** Just trying to convince the committee that my argument about *quantum linguistics* is, uh, legitimate. (She pushes the paper to show an outline.)\\n\\n**Alex:** Oh? That’s an… unexpected title. If that’s a thing, I love it. (He leans in, intrigued.) Tell me more.\\n\\n**Maya:** Well, it’s... Okay, that\\'s a lot. I’ll keep it to one point: language carries quantum information, and our interpretations of it affect how we think about reality. But— (she fumbles, then decides it\\'s a conversation starter) - I know you’re a traveler, right? I mean, you\\'re in a foreign city with a coffee in hand and a thesis in mind. How many cities, Alex?\\n\\n**Alex:** This is my second time in the city. First was, interestingly, a weekend of architecture tours. (He chuckles.) But I\\'m a long-time commuter. I fly around for work. Mostly tech stuff, you know: data pipelines. No quantum linguistics for me, but I do read a lot of science fiction.\\n\\n**Maya:** (Eyes widen.) Seriously, you\\'re a tech guy? I never would have guessed. How do you keep your mind sharp between all the code and deadlines?\\n\\n**Alex:** I have a ritual. Every Sunday before the week starts, I walk up a hill near my apartment. I leave a note on a swing and write... something like \"Think about the weight of this world while sprinting through code.\"\\n\\n**Maya:** (Laughs) That sounds… weird and brilliant. I\\'ve never thought about my thesis in that way.  \\n\\n**Alex:** Yeah, a lot of people think a thesis is just words—mine would be a *path*. By the way, do you... need a pen? Maybe you want to throw in a doodle?  \\n\\n**Maya:** (Shakes her head.) No, I think I’m good. But thank you for the coffee. (She pays with a card and wipes her fingers.)  \\n\\n**Alex:** Anytime. If you ever want to chat between your own and quantum thoughts, just let me know. I\\'m only a seat away.  \\n\\n**Maya:** (Smiles genuinely.) Thanks, Alex. It feels like… I already found a friend for the unexpected.  \\n\\n**Alex:** That’s what this city is for—meeting strangers on the first day and turning them into confederates.  \\n\\n**Maya:** ...okay, I\\'ll take that. (She closes her notebook, looks out the window, and then, to her surprise, turns back to read a handwritten note on Alex\\'s palm) \"Think about the weight of this world.\"  \\n\\n**Alex:** (Laughs.) See? The world just got a little lighter.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content='We need to produce a dialogue between two characters meeting for the first time. The instruction is \"Write a dialogue between two characters meeting for the first time.\" We can give a creative dialogue. The answer should be a dialogue. Provide names, perhaps setting. Just dialogue. Let\\'s create something distinct; maybe an elderly woman and a young student meeting on a train? Or a detective and a victim? Let\\'s just choose something that is clear, like a café meeting or a park.\\n\\nWe want to show they are meeting.\\n\\nProbably incorporate some conflict or a reveal. The prompt didn\\'t specify content. So produce something interesting. Ensure it\\'s just dialogue, but we can identify who speaks. Just the lines, perhaps in quoted form? I\\'ll just use label with name.\\n\\nLet\\'s write. We\\'ll present as: Alex: ...; Maya: ... etc.\\n\\nOk.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN_2\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a dialogue between two characters meeting for the first time.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84bda9",
   "metadata": {},
   "source": [
    "# Creating a rag db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07d19063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sqlite_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b402e2",
   "metadata": {},
   "source": [
    "# Parsing using llamaparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c193ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id fc6b51ce-9df5-4e27-ba75-606609192dd2\n",
      "."
     ]
    }
   ],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "  # See how to get your API key at https://docs.cloud.llamaindex.ai/api_key\n",
    "  api_key=os.environ[\"LLAMAPARSE_API\"],\n",
    "\n",
    "  # The parsing mode\n",
    "  parse_mode=\"parse_page_with_llm\",\n",
    "\n",
    "  # Whether to use high resolution OCR (Slow)\n",
    "  high_res_ocr=True,\n",
    "\n",
    "  # Adaptive long table. LlamaParse will try to detect long table and adapt the output\n",
    "  adaptive_long_table=True,\n",
    "\n",
    "  # Whether to try to extract outlined tables\n",
    "  outlined_table_extraction=True,\n",
    "\n",
    "  # Whether to output tables as HTML in the markdown output\n",
    "  output_tables_as_HTML=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# sync\n",
    "result = parser.parse(\"/Users/himanshu/projects/journalling-assitant/Books/Medium Discourses/Middle-Discourses-sujato-2025-08-25-1.pdf\")\n",
    "\n",
    "# sync batch\n",
    "# results = parser.parse([\"./my_file1.pdf\", \"./my_file2.pdf\"])\n",
    "\n",
    "# async\n",
    "# result = await parser.aparse(\"./my_file.pdf\")\n",
    "\n",
    "# async batch\n",
    "# results = await parser.aparse([\"./my_file1.pdf\", \"./my_file2.pdf\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96251cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: dd486cb9-a118-4413-b792-52d1389d2a56\n",
      "Text: # Middle Discourses  # Bhikkhu Sujato\n",
      "Doc ID: 9a36baa9-e1ea-40b4-b96a-3e29c25aa5a9\n",
      "Text: NO_CONTENT_HERE\n",
      "Doc ID: 34e59411-5e5d-42f9-872c-998b75ab1f47\n",
      "Text: M I D D L E D I S C O U R S E S  # A lucid translation of the\n",
      "Majjhima Nikāya  translated and introduced by Bhikkhu Sujato  # Volume\n",
      "1  # MN 1–50  # The First Fifty  Mūl a paṇṇā s a  0 SuttaCentral\n",
      "Doc ID: 8586e42d-4ef2-4f92-9a03-604d6b67d389\n",
      "Text: # Middle Discourses  # A translation of the Majjhimanikāya by\n",
      "Bhikkhu Sujato  Creative Commons Zero (CC0)  To the extent possible\n",
      "under law, Bhikkhu Sujato has waived all copyright and related or\n",
      "neighboring rights to Middle Discourses.  This work is published from\n",
      "Australia. This translation is an expression of an ancient spiritual\n",
      "text that ha...\n",
      "Doc ID: 0d29cd97-305e-4a27-a6df-09a6510bc6b5\n",
      "Text: The sage at peace is not reborn, does not grow old, and does not\n",
      "die. They are not shaken, and do not yearn. For they have nothing\n",
      "which would cause them to be reborn. Not being reborn, how could they\n",
      "grow old? Not growing old, how could they die? Not dying, how could\n",
      "they be shaken? Not shaking, for what could they yearn?  # “The\n",
      "Analysis of th...\n",
      "Doc ID: 8c74b16c-99d4-4e11-b96c-c44442aa5651\n",
      "Text: KAMMASADAMMA  Ca  Serhs # JAMBUDIPA  c.500 BCE  # MADHURA  #\n",
      "SUASENA  MACHAE  $949  Ps  $\\ra{}$  D VANASAG  pP  $Asa}$  # UJJENI  #\n",
      "GONADDHA  AVANTI  9999g4s  MAHISSATI  ASSAKA  # BAVARI'S HERMITAGE\n",
      "PATITTHANA\n",
      "Doc ID: 4192ac6f-4efa-47ae-bac2-814deecd7c9c\n",
      "Text: SAKYA  # SETAVYA 49  # KAPILAVATTHU  # SAKETA  # LUMBINI  #\n",
      "VIDEHA  # KOSALA  # KUSINARA  # MITHILA  # PAVA  # MALLA  # KESAPUTTA\n",
      "# VESALI  # BARANASI  # AYOJHA  # APANA  # KOSAMBI  # KASI  # GAYA  #\n",
      "ORAJAGAHA  # CAMPA  # 34s  # 9949941 ASr! 4  # 99  # 9  # 4  # 89  #\n",
      "A99  # DHA  # He  # 9  # 9  # 9  # o  # 100  # 150  # 200  # 150  #\n",
      "300 mi  #...\n",
      "Doc ID: d63f39b1-ace7-4057-843d-1a57d3273dd3\n",
      "Text: NO_CONTENT_HERE\n",
      "Doc ID: 9f47c189-b4f0-4686-81e2-a012c48c9a70\n",
      "Text: # Contents  # The SuttaCentral Editions Series  xiii  # Preface\n",
      "to Middle Discourses  xiv  # The Middle Discourses: conversations on\n",
      "matters of deep truth  xvii  # Acknowledgements  xliii  # Summary of\n",
      "Contents  xliv  # The First Fifty  <table> <tbody><tr> <td>MN 1</td>\n",
      "<td>The Root of All Things (Mūlapariyāyasutta)</td> <td>3</td> </tr>\n",
      "<tr> <t...\n",
      "Doc ID: 6df85779-d93b-4e9c-8fea-5a77dfb95b3a\n",
      "Text: # The Chapter on the Lion’s Roar  # MN 11  The Shorter Discourse\n",
      "on the Lion’s Roar (Cūḷasīhanādasutta) 105  # MN 12  The Longer\n",
      "Discourse on the Lion’s Roar (Mahāsīhanādasutta) 110  # MN 13  The\n",
      "Longer Discourse on the Mass of Suffering (Mahādukkhakkhandhasutta)\n",
      "131  # MN 14  The Shorter Discourse on the Mass of Suffering\n",
      "(Cūḷadukkhakkhandhasut...\n"
     ]
    }
   ],
   "source": [
    "# get the llama-index markdown documents\n",
    "markdown_documents = result.get_markdown_documents(split_by_page=True)\n",
    "\n",
    "# # get the llama-index text documents\n",
    "# text_documents = result.get_text_documents(split_by_page=False)\n",
    "\n",
    "# # get the image documents\n",
    "# image_documents = result.get_image_documents(\n",
    "#     include_screenshot_images=True,\n",
    "#     include_object_images=False,\n",
    "#     # Optional: download the images to a directory\n",
    "#     # (default is to return the image bytes in ImageDocument objects)\n",
    "#     image_download_dir=\"./images\",\n",
    "# )\n",
    "\n",
    "# access the raw job result\n",
    "# Items will vary based on the parser configuration\n",
    "for page in markdown_documents[:10]:\n",
    "    print(page)\n",
    "    # print(page.md)\n",
    "    # print(page.images)\n",
    "    # print(page.layout)\n",
    "    # print(page.structuredData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e6cf1",
   "metadata": {},
   "source": [
    "# Converting html to markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6546393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted HTML to Markdown!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# The URL of the page you want to convert\n",
    "# Example: a page from Access to Insight containing Buddhist texts\n",
    "url = 'https://raw.githubusercontent.com/suttacentral/editions/main/en/sujato/dn/html/Long-Discourses-sujato-2025-08-25.html'\n",
    "\n",
    "try:\n",
    "    # Step 1: Fetch the HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises an exception for bad status codes (4xx or 5xx)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Step 2: Convert the HTML to Markdown\n",
    "    # The 'heading_style=\"ATX\"' argument ensures headings are created with '#'\n",
    "    markdown_text = md(html_content, heading_style=\"ATX\")\n",
    "\n",
    "    # Step 3: Save the result to a file\n",
    "    with open('dhammapada_chapter1.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_text)\n",
    "\n",
    "    print(\"Successfully converted HTML to Markdown!\")\n",
    "    # print(\"\\n--- Preview ---\\n\")\n",
    "    # print(markdown_text[:500]) # Print the first 500 characters as a preview\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching the URL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37044fd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3baf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7037205",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PGVector\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# from Pipeline.config import *\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import PGVector\n",
    "# from Pipeline.config import *\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import AzureBlobStorageFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import asyncio\n",
    "import datetime\n",
    "import logging\n",
    "import base64\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import datetime\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "\n",
    "### Define chunking function \n",
    "\n",
    "\n",
    "@observe(capture_input=True, capture_output=True)\n",
    "async def semantically_chunking(text):\n",
    "    \"\"\"Chunk text into smaller pieces using a rolling window splitter of semantic router \"\"\"\n",
    "    try:\n",
    "        chunked_texts = []\n",
    "        encoder = OpenAIEncoder(name=\"text-embedding-ada-002\")\n",
    "        chunker = StatisticalChunker(encoder=encoder,min_split_tokens=200,max_split_tokens=2000,window_size=2)\n",
    "        chunks_async = await chunker.acall(docs=[text])\n",
    "        for chunk in chunks_async[0]:\n",
    "            chunk_text = \"\"\n",
    "            for split in chunk.splits:\n",
    "                chunk_text += split\n",
    "            ## Remove null characters from the text\n",
    "            if '\\x00' in chunk_text:\n",
    "                    chunk_text=text.replace('\\x00', '')\n",
    "            chunked_texts.append(chunk_text)\n",
    "        logging.info(f'Number of chunks for given text is {len(chunked_texts)}')\n",
    "        logging.info(\"*\"*100)\n",
    "        return chunked_texts\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in semantically_chunking: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "@observe(capture_input=True, capture_output=True)\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "async def markdown_chunking(text, max_chunk_size=1000, level=1, max_level=6):\n",
    "\n",
    "    \"\"\"\n",
    "    Objective: Chunk text into smaller pieces using markdown splitter.\n",
    "\n",
    "\n",
    "\n",
    "    Inputs:\n",
    "    - text: str: Text to be chunked\n",
    "    - max_chunk_size: int: Maximum size of each chunk\n",
    "    - level: int: Current header level\n",
    "    - max_level: int: Maximum header level to split on\n",
    "\n",
    "    Workflow and Steps:\n",
    "\n",
    "    1. Split the text into smaller chunks using markdown splitter.\n",
    "    2. If the chunk is larger than the max_chunk_size and the current level is less than the max_level, recursively call the markdown_chunking function with the chunk as input.\n",
    "    3. If the chunk is smaller than the max_chunk_size or the current level is equal to the max_level, keep the chunk as is.\n",
    "    4. Return the list of chunks.\n",
    "\n",
    "    Outputs:\n",
    "    - docs: list: List of Document objects, each containing a chunk of text\n",
    "    - level: int: Current header level\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def add_header_level(level):\n",
    "        return [((\"#\" * i), f\"Header {i}\") for i in range(1, level + 1)]\n",
    "    \n",
    "    \n",
    "    def get_headers_string(chunk_metadata):\n",
    "        header_string=\"\"\n",
    "        for header in chunk_metadata:\n",
    "            header_level=header[-1]\n",
    "            header_text=chunk_metadata[header]\n",
    "            header_string+=f\"{'#'*int(header_level)} {header_text}\\n\"\n",
    "        return header_string\n",
    "    \n",
    "\n",
    "    def remove_existing_headers(content: str) -> str:\n",
    "        # Remove header lines (lines starting with #)\n",
    "        return re.sub(r'^#+\\s.*$', '', content, flags=re.MULTILINE).strip()\n",
    "\n",
    "    headers_to_split_on = add_header_level(level)\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "        )\n",
    "    \n",
    "    md_header_splits = markdown_splitter.split_text(text)\n",
    "\n",
    "    docs = []\n",
    "    for doc in md_header_splits:\n",
    "        if len(doc.page_content) > max_chunk_size and level < max_level:\n",
    "\n",
    "            new_chunks, _ = await markdown_chunking(doc.page_content, max_chunk_size, level + 1, max_level)\n",
    "\n",
    "            ## Prepand each chunk with header from metadata\n",
    "            new_chunks = [Document(get_headers_string(chunk.metadata)+remove_existing_headers(chunk.page_content),metadata=chunk.metadata) for chunk in new_chunks]\n",
    "            docs.extend(new_chunks)\n",
    "        else:\n",
    "            # If chunk is small enough or we've reached max level, keep it as is\n",
    "\n",
    "            docs.append(Document(get_headers_string(doc.metadata)+remove_existing_headers(doc.page_content), metadata=doc.metadata))\n",
    "    \n",
    "    return docs, level\n",
    "\n",
    "@observe(capture_input=False, capture_output=False)\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "async def chunk_text(text,method='semantic'):\n",
    "    \"\"\"\n",
    "    Wrapper function to chunk text using either semantic router or markdown splitter.\n",
    "\n",
    "    \"\"\"\n",
    "    if method=='semantic':\n",
    "        logging.info(\"Chunking text using semantic router\")\n",
    "        return await semantically_chunking(text)\n",
    "        \n",
    "    elif method=='markdown':\n",
    "        logging.info(\"Chunking text using markdown splitter\")\n",
    "        langchain_docs, level = await markdown_chunking(text)\n",
    "        ## Get the text from the langchain docs \n",
    "        chunks = [doc.page_content for doc in langchain_docs]\n",
    "        return chunks\n",
    "        \n",
    "    else:\n",
    "        logging.error(\"Invalid method for chunking\")\n",
    "        raise ValueError(\"Invalid method for chunking\")\n",
    "\n",
    "\n",
    "@observe(capture_input=False, capture_output=False)\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "async def embed_in_vdb(slug,texts,metadatas): \n",
    "\n",
    "    \"\"\"\n",
    "    Objective: Embed the text data and save it in relevant langchain collection in vector database.\n",
    "\n",
    "    Inputs:\n",
    "    - slug: str: Slug of the company, used to name the collection in the vector database\n",
    "    - texts: list: List of text data to be embedded\n",
    "    - metadatas: list: List of metadata associated with each text data\n",
    "\n",
    "    Outputs:\n",
    "    - created_object_list: list: List of created objects in the table\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    ## Embed the data and save it in langchain collection ## \n",
    "    CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}/{dbname}\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    COLLECTION_NAME=slug+\"_vdb\"\n",
    "    ## Connecting to vector store ##\n",
    "    vector_store = PGVector(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection_string=CONNECTION_STRING,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    created_object_list=await vector_store.aadd_texts(texts,metadatas)\n",
    "    # print(f'Created {len(created_object_list)} new embeddings in the table')\n",
    "    return created_object_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b947bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76012fdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqllite'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqllite\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sqllite'"
     ]
    }
   ],
   "source": [
    "import sqllite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfac6ee",
   "metadata": {},
   "source": [
    "# Trying out PAGEINDEXRAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519ac5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "journalling-assitant-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
